{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d49688-5d45-45be-8ca9-9b43f3f07116",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    classification_report, accuracy_score, roc_auc_score,\n",
    "    precision_recall_curve, confusion_matrix\n",
    ")\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# === Load and Prepare Dataset ===\n",
    "df = pd.read_csv(\"class_all_with_chronic_names.csv\")\n",
    "df = df[df['class'].isin([1, 2, 3])]\n",
    "df['class'] = df['class'].replace({1: 0, 2: 1, 3: 1})\n",
    "y = df['class']\n",
    "drop_cols = ['HASHED_PERSONID', 'ENCNTR_ID_SI', 'DIAG_DT_TM', 'ICD', 'DIAGNOSIS_DISPLAY', 'DIAG_TYPE']\n",
    "X = df.drop(columns=drop_cols + ['class'], errors='ignore')\n",
    "\n",
    "# === Encode Categorical ===\n",
    "cat_cols = X.select_dtypes(include=['object']).columns\n",
    "for col in cat_cols:\n",
    "    le = LabelEncoder()\n",
    "    X[col] = le.fit_transform(X[col].astype(str))\n",
    "\n",
    "# === Impute & Select Features ===\n",
    "X = pd.DataFrame(SimpleImputer(strategy=\"mean\").fit_transform(X), columns=X.columns)\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf.fit(X, y)\n",
    "top_features = pd.Series(rf.feature_importances_, index=X.columns).nlargest(40).index.tolist()\n",
    "X = X[top_features]\n",
    "\n",
    "# === Train-Validation-Test Split ===\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, stratify=y, test_size=0.2, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, stratify=y_train_val, test_size=0.25, random_state=42)\n",
    "\n",
    "# === Compute Class Weights and Scale_Pos_Weight ===\n",
    "class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y_train), y=y_train)\n",
    "sample_weights = np.where(y_train == 0, class_weights[0], class_weights[1])\n",
    "num_class0 = sum(y_train == 0)\n",
    "num_class1 = sum(y_train == 1)\n",
    "scale_pos_weight = num_class0 / num_class1\n",
    "\n",
    "# === Train XGBoost Model ===\n",
    "base_model = xgb.XGBClassifier(\n",
    "    n_estimators=200,\n",
    "    learning_rate=0.03,\n",
    "    max_depth=5,\n",
    "    subsample=0.6,\n",
    "    colsample_bytree=0.6,\n",
    "    reg_lambda=10,\n",
    "    reg_alpha=5,\n",
    "    scale_pos_weight=scale_pos_weight,\n",
    "    objective='binary:logistic',\n",
    "    use_label_encoder=False,\n",
    "    eval_metric='auc',\n",
    "    random_state=42\n",
    ")\n",
    "base_model.fit(X_train, y_train, sample_weight=sample_weights)\n",
    "\n",
    "# === Calibrate Model ===\n",
    "calibrated_model = CalibratedClassifierCV(base_model, method='sigmoid', cv='prefit')\n",
    "calibrated_model.fit(X_val, y_val)\n",
    "\n",
    "# === Evaluation Function ===\n",
    "def evaluate(model, X, y, label, threshold=0.636):\n",
    "    y_prob = model.predict_proba(X)[:, 1]\n",
    "    y_pred = (y_prob >= threshold).astype(int)\n",
    "    print(f\"\\nðŸ“Š {label} Report (Threshold={threshold:.3f}):\")\n",
    "    print(classification_report(y, y_pred))\n",
    "    print(f\"âœ… Accuracy: {accuracy_score(y, y_pred):.4f}\")\n",
    "    print(f\"ðŸŽ¯ ROC AUC: {roc_auc_score(y, y_prob):.4f}\")\n",
    "    return y, y_prob, y_pred\n",
    "\n",
    "# === Evaluate with Threshold = 0.636 ===\n",
    "THRESH = 0.636\n",
    "y_train_true, y_train_prob, _ = evaluate(calibrated_model, X_train, y_train, \"Train\", THRESH)\n",
    "y_val_true, y_val_prob, _ = evaluate(calibrated_model, X_val, y_val, \"Validation\", THRESH)\n",
    "y_test_true, y_test_prob, y_test_pred = evaluate(calibrated_model, X_test, y_test, \"Test\", THRESH)\n",
    "\n",
    "# === Confusion Matrix ===\n",
    "cm = confusion_matrix(y_test_true, y_test_pred)\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=['Non-Worsening', 'Worsening'],\n",
    "            yticklabels=['Non-Worsening', 'Worsening'])\n",
    "plt.title(f\"Confusion Matrix - XGBoost)\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "def plot_roc(y_true, y_prob, label):\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_prob)\n",
    "    auc_score = roc_auc_score(y_true, y_prob)\n",
    "    plt.plot(fpr, tpr, label=f\"{label} AUC = {auc_score:.4f}\")\n",
    "\n",
    "# === Plot ROC for All Sets ===\n",
    "plt.figure(figsize=(8, 6))\n",
    "plot_roc(y_train_true, y_train_prob, \"Train\")\n",
    "plot_roc(y_val_true, y_val_prob, \"Validation\")\n",
    "plot_roc(y_test_true, y_test_prob, \"Test\")\n",
    "plt.plot([0, 1], [0, 1], 'k--', label='Random Guess')\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curve\")\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
