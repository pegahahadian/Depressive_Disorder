{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef293a9f-dddc-444b-907e-58c466d3e6af",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Trustworthy-AI ReAct-Style Model Judge (HF Transformers version; CPU-safe)\n",
    "- Deterministic selection: hard constraints + composite score.\n",
    "- LLM Audit Note via Hugging Face `pipeline()` (TinyLlama by default).\n",
    "- Works in Jupyter or CLI.\n",
    "\n",
    "Run (Jupyter): just execute the cell.\n",
    "Run (CLI):  python judge_react_hf.py --policy default --models_path artifacts/models.json\n",
    "\n",
    "If the HF model load fails (env issues, missing pkgs, no token), you'll still get a sane\n",
    "deterministic audit note (fallback).\n",
    "\"\"\"\n",
    "\n",
    "import os, sys, json, argparse, traceback\n",
    "from pathlib import Path\n",
    "\n",
    "# SECTION 0 — MODELS & POLICY\n",
    "DEFAULT_MODELS = [\n",
    "  {\n",
    "    \"model_name\": \"XGBoost\",\n",
    "    \"sets\": {\n",
    "      \"train\": {\"accuracy\": 0.8087, \"roc_auc\": 0.8880, \"recall_pos\": 0.81, \"recall_neg\": 0.81, \"f1_weighted\": 0.81, \"n\": 460},\n",
    "      \"val\":   {\"accuracy\": 0.6948, \"roc_auc\": 0.6785, \"recall_pos\": 0.75, \"recall_neg\": 0.59, \"f1_weighted\": 0.70, \"n\": 154},\n",
    "      \"test\":  {\"accuracy\": 0.7208, \"roc_auc\": 0.7765, \"recall_pos\": 0.77, \"recall_neg\": 0.63, \"f1_weighted\": 0.72, \"n\": 154}\n",
    "    }\n",
    "  },\n",
    "  {\n",
    "    \"model_name\": \"CatBoost\",\n",
    "    \"sets\": {\n",
    "      \"train\": {\"accuracy\": 0.7543, \"roc_auc\": 0.8619, \"recall_pos\": 0.72, \"recall_neg\": 0.83, \"f1_weighted\": 0.76, \"n\": 460},\n",
    "      \"val\":   {\"accuracy\": 0.6429, \"roc_auc\": 0.6630, \"recall_pos\": 0.66, \"recall_neg\": 0.61, \"f1_weighted\": 0.65, \"n\": 154},\n",
    "      \"test\":  {\"accuracy\": 0.7100, \"roc_auc\": 0.7600, \"recall_pos\": 0.72, \"recall_neg\": 0.69, \"f1_weighted\": 0.71, \"n\": 154}\n",
    "    }\n",
    "  },\n",
    "  {\n",
    "    \"model_name\": \"LogisticRegression\",\n",
    "    \"sets\": {\n",
    "      \"train\": {\"accuracy\": 0.8055, \"roc_auc\": 0.8491, \"recall_pos\": 0.88, \"recall_neg\": 0.70, \"f1_weighted\": 0.80, \"n\": 365},\n",
    "      \"val\":   {\"accuracy\": 0.7623, \"roc_auc\": 0.8569, \"recall_pos\": 0.89, \"recall_neg\": 0.59, \"f1_weighted\": 0.75, \"n\": 122},\n",
    "      \"test\":  {\"accuracy\": 0.7131, \"roc_auc\": 0.7973, \"recall_pos\": 0.79, \"recall_neg\": 0.61, \"f1_weighted\": 0.71, \"n\": 122}\n",
    "    }\n",
    "  },\n",
    "  {\n",
    "    \"model_name\": \"RandomForest\",\n",
    "    \"sets\": {\n",
    "      \"train\": {\"accuracy\": 0.9890, \"roc_auc\": 0.9996, \"recall_pos\": 1.00, \"recall_neg\": 0.98, \"f1_weighted\": 0.99, \"n\": 365},\n",
    "      \"val\":   {\"accuracy\": 0.9836, \"roc_auc\": 0.9997, \"recall_pos\": 1.00, \"recall_neg\": 0.96, \"f1_weighted\": 0.98, \"n\": 122},\n",
    "      \"test\":  {\"accuracy\": 0.6967, \"roc_auc\": 0.7216, \"recall_pos\": 0.85, \"recall_neg\": 0.49, \"f1_weighted\": 0.69, \"n\": 122}\n",
    "    }\n",
    "  },\n",
    "  {\n",
    "    \"model_name\": \"DeepNeuralNetwork\",\n",
    "    \"sets\": {\n",
    "      \"train\": {\"accuracy\": 0.7753, \"roc_auc\": 0.8559, \"recall_pos\": 0.87, \"recall_neg\": 0.64, \"f1_weighted\": 0.77, \"n\": 365},\n",
    "      \"val\":   {\"accuracy\": 0.7049, \"roc_auc\": 0.7028, \"recall_pos\": 0.82, \"recall_neg\": 0.55, \"f1_weighted\": 0.70, \"n\": 122},\n",
    "      \"test\":  {\"accuracy\": 0.6393, \"roc_auc\": 0.6708, \"recall_pos\": 0.73, \"recall_neg\": 0.51, \"f1_weighted\": 0.64, \"n\": 122}\n",
    "    }\n",
    "  }\n",
    "]\n",
    "\n",
    "POLICIES = {\n",
    "  \"default\": {\n",
    "    \"notes\": \"Sensitivity and AUC are required. Optional trustworthiness terms if provided.\",\n",
    "    \"hard\": {\"min_recall_pos\": 0.70, \"min_roc_auc\": 0.70},\n",
    "    \"weights\": {\n",
    "      \"auc\": 50, \"f1\": 20, \"specificity\": 10, \"accuracy\": 10, \"overfit_penalty\": 10,\n",
    "      \"calib_ece\": 3, \"calib_slope_dev\": 2, \"fairness_eq_odds_gap\": 10,\n",
    "      \"pr_auc\": 5, \"dca_net_benefit\": 5, \"conformal_coverage\": 5, \"conformal_set_size\": 3,\n",
    "      \"stress_missing10\": 2, \"stress_missing30\": 3, \"stress_labelnoise5\": 2,\n",
    "      \"stability_auc_sd\": 3\n",
    "    }\n",
    "  },\n",
    "  \"high_sensitivity\": {\n",
    "    \"notes\": \"Stricter sensitivity floor.\",\n",
    "    \"hard\": {\"min_recall_pos\": 0.80, \"min_roc_auc\": 0.70},\n",
    "    \"weights\": {\"auc\": 45, \"f1\": 20, \"specificity\": 10, \"accuracy\": 10, \"overfit_penalty\": 15}\n",
    "  }\n",
    "}\n",
    "\n",
    "# SECTION 1 — DETERMINISTIC JUDGE\n",
    "def _mean(xs): return sum(xs)/len(xs) if xs else 0.0\n",
    "def _stdev(xs):\n",
    "  if len(xs) < 2: return 0.0\n",
    "  mu = _mean(xs)\n",
    "  return ((sum((x-mu)**2 for x in xs)/(len(xs)-1))**0.5)\n",
    "\n",
    "def check_constraints(model, policy):\n",
    "  t = model[\"sets\"][\"test\"]\n",
    "  hard = policy[\"hard\"]\n",
    "  violations = []\n",
    "  if \"min_recall_pos\" in hard and t.get(\"recall_pos\") is not None and t[\"recall_pos\"] < hard[\"min_recall_pos\"]:\n",
    "    violations.append(f\"sensitivity {t['recall_pos']:.2f} < {hard['min_recall_pos']:.2f}\")\n",
    "  if \"min_roc_auc\" in hard and t.get(\"roc_auc\") is not None and t[\"roc_auc\"] < hard[\"min_roc_auc\"]:\n",
    "    violations.append(f\"ROC-AUC {t['roc_auc']:.3f} < {hard['min_roc_auc']:.3f}\")\n",
    "\n",
    "  extras = model.get(\"extras\", {}) or {}\n",
    "  fair   = extras.get(\"fairness\", {}) or {}\n",
    "  calib  = extras.get(\"calibration\", {}) or {}\n",
    "  uncert = extras.get(\"uncertainty\", {}) or {}\n",
    "\n",
    "  if \"max_eq_odds_gap\" in hard and fair.get(\"equalized_odds_gap\") is not None and fair[\"equalized_odds_gap\"] > hard[\"max_eq_odds_gap\"]:\n",
    "    violations.append(f\"EO gap {fair['equalized_odds_gap']:.3f} > {hard['max_eq_odds_gap']:.3f}\")\n",
    "  if \"slope_min\" in hard and calib.get(\"slope\") is not None and calib[\"slope\"] < hard[\"slope_min\"]:\n",
    "    violations.append(f\"calibration slope {calib['slope']:.2f} < {hard['slope_min']:.2f}\")\n",
    "  if \"slope_max\" in hard and calib.get(\"slope\") is not None and calib[\"slope\"] > hard[\"slope_max\"]:\n",
    "    violations.append(f\"calibration slope {calib['slope']:.2f} > {hard['slope_max']:.2f}\")\n",
    "  if \"min_conformal_coverage\" in hard and uncert.get(\"coverage\") is not None and uncert[\"coverage\"] < hard[\"min_conformal_coverage\"]:\n",
    "    violations.append(f\"conformal coverage {uncert['coverage']:.2f} < {hard['min_conformal_coverage']:.2f}\")\n",
    "\n",
    "  return {\"pass\": len(violations)==0, \"violations\": violations}\n",
    "\n",
    "def overfit_gap(model):\n",
    "  s = model[\"sets\"]\n",
    "  tr, te = s.get(\"train\", {}), s.get(\"test\", {})\n",
    "  if tr.get(\"roc_auc\") is not None and te.get(\"roc_auc\") is not None:\n",
    "    return max(0.0, tr[\"roc_auc\"] - te[\"roc_auc\"])\n",
    "  return 0.0\n",
    "\n",
    "def _sigmoid_norm(x, mean, sd):\n",
    "  if x is None: return 0.0\n",
    "  if sd and sd > 0:\n",
    "    z = (x - mean)/sd\n",
    "    return 1/(1 + 2.718281828**(-z))\n",
    "  return max(0.0, min(1.0, (x - 0.5)/0.5))\n",
    "\n",
    "def score_model(model, policy, cohort_baselines):\n",
    "  w = policy[\"weights\"]\n",
    "  t = model[\"sets\"][\"test\"]\n",
    "  extras = model.get(\"extras\", {}) or {}\n",
    "\n",
    "  auc  = t.get(\"roc_auc\"); f1 = t.get(\"f1_weighted\"); spec = t.get(\"recall_neg\"); acc = t.get(\"accuracy\")\n",
    "  gap  = overfit_gap(model)\n",
    "\n",
    "  auc_comp = _sigmoid_norm(auc, cohort_baselines[\"auc_mean\"], cohort_baselines[\"auc_sd\"])\n",
    "  f1_comp  = _sigmoid_norm(f1,  cohort_baselines[\"f1_mean\"],  cohort_baselines[\"f1_sd\"])\n",
    "  spec_comp= 0.0 if spec is None else spec\n",
    "  acc_comp = 0.0 if acc is None else acc\n",
    "  pen      = max(0.0, 1.0 - min(gap/0.30, 1.0))  # gap 0..0.3 -> 1..0\n",
    "\n",
    "  total = (w.get(\"auc\",0)*auc_comp + w.get(\"f1\",0)*f1_comp +\n",
    "           w.get(\"specificity\",0)*spec_comp + w.get(\"accuracy\",0)*acc_comp +\n",
    "           w.get(\"overfit_penalty\",0)*pen)\n",
    "\n",
    "  subs = {\n",
    "    \"auc_component\": auc_comp * w.get(\"auc\",0),\n",
    "    \"f1_component\": f1_comp * w.get(\"f1\",0),\n",
    "    \"specificity_component\": spec_comp * w.get(\"specificity\",0),\n",
    "    \"accuracy_component\": acc_comp * w.get(\"accuracy\",0),\n",
    "    \"overfit_component\": pen * w.get(\"overfit_penalty\",0),\n",
    "    \"overfit_gap\": gap\n",
    "  }\n",
    "\n",
    "  # Optional extras (won't matter unless provided)\n",
    "  calib = (extras.get(\"calibration\", {}) or {})\n",
    "  ece = calib.get(\"ece\"); slope = calib.get(\"slope\")\n",
    "  slope_dev = abs(1 - slope) if isinstance(slope, (int,float)) else None\n",
    "  if \"calib_ece\" in w and ece is not None:\n",
    "    ece_score = max(0.0, 1.0 - min(ece/0.20, 1.0))\n",
    "    total += w[\"calib_ece\"] * ece_score; subs[\"calib_ece_component\"] = ece_score * w[\"calib_ece\"]; subs[\"ece\"] = ece\n",
    "  if \"calib_slope_dev\" in w and slope_dev is not None:\n",
    "    slope_score = max(0.0, 1.0 - min(slope_dev/0.30, 1.0))\n",
    "    total += w[\"calib_slope_dev\"] * slope_score; subs[\"calib_slope_component\"] = slope_score * w[\"calib_slope_dev\"]; subs[\"slope_dev\"] = slope_dev\n",
    "\n",
    "  fair = (extras.get(\"fairness\", {}) or {})\n",
    "  eq_odds_gap = fair.get(\"equalized_odds_gap\")\n",
    "  if \"fairness_eq_odds_gap\" in w and eq_odds_gap is not None:\n",
    "    fair_score = max(0.0, 1.0 - min(eq_odds_gap/0.20, 1.0))\n",
    "    total += w[\"fairness_eq_odds_gap\"] * fair_score; subs[\"fairness_component\"] = fair_score * w[\"fairness_eq_odds_gap\"]; subs[\"eq_odds_gap\"] = eq_odds_gap\n",
    "\n",
    "  pr_auc = extras.get(\"pr_auc\")\n",
    "  if \"pr_auc\" in w and pr_auc is not None:\n",
    "    pr_comp = max(0.0, min(1.0, pr_auc))\n",
    "    total += w[\"pr_auc\"] * pr_comp; subs[\"pr_auc_component\"] = pr_comp * w[\"pr_auc\"]\n",
    "\n",
    "  dca = (extras.get(\"dca\", {}) or {})\n",
    "  nb = dca.get(\"avg_net_benefit_10_30\")\n",
    "  if \"dca_net_benefit\" in w and nb is not None:\n",
    "    nb_score = max(0.0, min(1.0, (nb + 0.01)/0.06))\n",
    "    total += w[\"dca_net_benefit\"] * nb_score; subs[\"dca_component\"] = nb_score * w[\"dca_net_benefit\"]\n",
    "\n",
    "  un = (extras.get(\"uncertainty\", {}) or {})\n",
    "  cov = un.get(\"coverage\"); setsize = un.get(\"avg_set_size\")\n",
    "  if \"conformal_coverage\" in w and cov is not None:\n",
    "    cov_score = max(0.0, min(1.0, (cov - 0.8)/0.2))\n",
    "    total += w[\"conformal_coverage\"] * cov_score; subs[\"conformal_coverage_component\"] = cov_score * w[\"conformal_coverage\"]\n",
    "  if \"conformal_set_size\" in w and setsize is not None:\n",
    "    ss_score = max(0.0, min(1.0, 1.0 - ((setsize - 1.0)/0.5)))\n",
    "    total += w[\"conformal_set_size\"] * ss_score; subs[\"conformal_set_size_component\"] = ss_score * w[\"conformal_set_size\"]\n",
    "\n",
    "  stab = (extras.get(\"stability\", {}) or {})\n",
    "  auc_sd = stab.get(\"auc_sd_over_seeds\")\n",
    "  if \"stability_auc_sd\" in w and auc_sd is not None:\n",
    "    ssd = max(0.0, 1.0 - min(auc_sd/0.02, 1.0))\n",
    "    total += w[\"stability_auc_sd\"] * ssd; subs[\"stability_auc_sd_component\"] = ssd * w[\"stability_auc_sd\"]\n",
    "\n",
    "  return {\"total\": float(total), \"subs\": subs}\n",
    "\n",
    "def _reject_explanation(model_name, t, policy, violations):\n",
    "  hard = policy[\"hard\"]\n",
    "  parts = []\n",
    "  sens = t.get(\"recall_pos\"); auc = t.get(\"roc_auc\")\n",
    "  if any(\"sensitivity\" in v for v in violations) and sens is not None and \"min_recall_pos\" in hard:\n",
    "    parts.append(f\"sensitivity {sens:.2f} fell below the floor {hard['min_recall_pos']:.2f}\")\n",
    "  if any(\"ROC-AUC\" in v for v in violations) and auc is not None and \"min_roc_auc\" in hard:\n",
    "    parts.append(f\"AUC {auc:.3f} < minimum {hard['min_roc_auc']:.2f}\")\n",
    "  if not parts:\n",
    "    parts = [\", \".join(violations)]\n",
    "  return f\"- {model_name}: \" + \"; \".join(parts) + \".\"\n",
    "\n",
    "def _top_two_explanation(rank_items):\n",
    "  if len(rank_items) < 2: return None\n",
    "  r1, r2 = rank_items[0], rank_items[1]\n",
    "  m1, t1 = r1[\"model_name\"], r1[\"test\"]; m2, t2 = r2[\"model_name\"], r2[\"test\"]\n",
    "  why1 = (f\"{m1} ranked #1 for higher sensitivity ({t1.get('recall_pos',0):.2f} vs {t2.get('recall_pos',0):.2f}), \"\n",
    "          f\"and better discrimination (AUC {t1.get('roc_auc',0):.3f} vs {t2.get('roc_auc',0):.3f}, \"\n",
    "          f\"F1w {t1.get('f1_weighted',0):.2f} vs {t2.get('f1_weighted',0):.2f}).\")\n",
    "  why2 = (f\"{m2} ranked #2 with better specificity ({t2.get('recall_neg',0):.2f} vs {t1.get('recall_neg',0):.2f}), \"\n",
    "          f\"reducing false positives.\")\n",
    "  trade = (\"Trade-off: choose #1 for early identification (sensitivity); \"\n",
    "           \"choose #2 to reduce workload (specificity).\")\n",
    "  return why1, why2, trade\n",
    "\n",
    "def summarize(models, policy_name, cohort_n=None):\n",
    "  policy = POLICIES[policy_name]\n",
    "  if cohort_n is not None:\n",
    "    models = [m for m in models if m[\"sets\"][\"test\"].get(\"n\") == cohort_n]\n",
    "\n",
    "  aucs = [m[\"sets\"][\"test\"].get(\"roc_auc\") for m in models if m[\"sets\"][\"test\"].get(\"roc_auc\") is not None]\n",
    "  f1s  = [m[\"sets\"][\"test\"].get(\"f1_weighted\") for m in models if m[\"sets\"][\"test\"].get(\"f1_weighted\") is not None]\n",
    "  cohort = {\"auc_mean\": _mean(aucs), \"auc_sd\": _stdev(aucs), \"f1_mean\": _mean(f1s), \"f1_sd\": _stdev(f1s)}\n",
    "\n",
    "  ranking, rejects = [], []\n",
    "  for m in models:\n",
    "    cons = check_constraints(m, policy)\n",
    "    if not cons[\"pass\"]:\n",
    "      rejects.append({\n",
    "        \"model_name\": m[\"model_name\"],\n",
    "        \"violations\": cons[\"violations\"],\n",
    "        \"test\": m[\"sets\"][\"test\"],\n",
    "        \"explanation\": _reject_explanation(m[\"model_name\"], m[\"sets\"][\"test\"], policy, cons[\"violations\"])\n",
    "      })\n",
    "      continue\n",
    "    sc = score_model(m, policy, cohort)\n",
    "    ranking.append({\n",
    "      \"model_name\": m[\"model_name\"],\n",
    "      \"score\": sc[\"total\"],\n",
    "      \"subs\": sc[\"subs\"],\n",
    "      \"test\": m[\"sets\"][\"test\"]\n",
    "    })\n",
    "\n",
    "  ranking.sort(key=lambda x: x[\"score\"], reverse=True)\n",
    "  return {\"ranking\": ranking, \"rejects\": rejects, \"policy\": policy, \"cohort\": cohort, \"cohort_n\": cohort_n}\n",
    "\n",
    "# SECTION 2 — LLM AUDIT (HF TRANSFORMERS, CPU)\n",
    "SYSTEM_PROMPT = (\n",
    "  \"You are a Trustworthy-AI auditor for mental-health risk prediction. \"\n",
    "  \"Use only the provided metrics and ranking. Do not re-rank or change thresholds.\"\n",
    ")\n",
    "\n",
    "USER_TEMPLATE = \"\"\"Context:\n",
    "- Task: select a model to predict progression from mild to moderate/severe depression within 24 months.\n",
    "- Operating point: frozen from validation; test metrics are final.\n",
    "- Clinical priority: sensitivity floor applies.\n",
    "\n",
    "Policy (hard + weights):\n",
    "{policy_json}\n",
    "\n",
    "Deterministic judge output (top-k JSON):\n",
    "{judge_json}\n",
    "\n",
    "Instructions (follow exactly):\n",
    "1) Do NOT re-rank models or suggest threshold/training changes.\n",
    "2) In 5–7 bullet points, explain:\n",
    "   - Why Rank #1 is selected (cite exact numbers).\n",
    "   - How Rank #2 compares and the trade-off (sensitivity vs specificity).\n",
    "   - Material risks (calibration/fairness unknowns, overfit gap, dataset shift).\n",
    "   - What checks could flip the decision (external validation, DCA, subgroup parity).\n",
    "3) ≤1200 characters. No tables. No PHI.\n",
    "\"\"\"\n",
    "\n",
    "def _fallback_plain_audit(ranking):\n",
    "  if not ranking:\n",
    "    return \"Audit note: no surviving models after hard constraints; revisit floors/thresholds.\"\n",
    "  r1 = ranking[0]; t1 = r1[\"test\"]; m1 = r1[\"model_name\"]\n",
    "  out = [f\"Selected {m1} for sensitivity {t1.get('recall_pos',0):.2f} and AUC {t1.get('roc_auc',0):.3f} \"\n",
    "         f\"(Acc {t1.get('accuracy',0):.2f}, F1w {t1.get('f1_weighted',0):.2f}).\"]\n",
    "  if len(ranking) > 1:\n",
    "    r2 = ranking[1]; t2 = r2[\"test\"]; m2 = r2[\"model_name\"]\n",
    "    out.append(f\"{m2} is next-best with specificity {t2.get('recall_neg',0):.2f} \"\n",
    "               f\"(AUC {t2.get('roc_auc',0):.3f}, Sens {t2.get('recall_pos',0):.2f}).\")\n",
    "    out.append(\"Trade-off: prefer #1 for early ID (sensitivity) vs #2 for workload (specificity).\")\n",
    "  out.append(\"Risks: calibration/fairness unknowns; potential overfit; dataset shift.\")\n",
    "  out.append(\"Next checks: external holdouts (OCHIN/MedStar), DCA/net benefit, subgroup parity.\")\n",
    "  return \"- \" + \"\\n- \".join(out)\n",
    "\n",
    "def _build_prompt(policy, ranking, top_k=2):\n",
    "  top = ranking[:max(1, top_k)]\n",
    "  policy_json = json.dumps(policy, indent=2)\n",
    "  judge_json  = json.dumps({\"ranking\": top}, indent=2)\n",
    "  return (\n",
    "    f\"System:\\n{SYSTEM_PROMPT}\\n\\nUser:\\n\" +\n",
    "    USER_TEMPLATE.format(policy_json=policy_json, judge_json=judge_json) +\n",
    "    \"\\nAssistant:\\n- \"\n",
    "  )\n",
    "\n",
    "def llm_audit_note_via_hf(policy, ranking, model_id=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
    "                          temperature=0.2, max_new_tokens=280, top_k=2):\n",
    "  \"\"\"\n",
    "  LLM note via HF transformers pipeline (CPU only). Prompts for HF token first.\n",
    "  Falls back to deterministic text if any step fails.\n",
    "  \"\"\"\n",
    "  if not ranking:\n",
    "    return \"Audit note: no surviving models after hard constraints; revisit floors/thresholds.\"\n",
    "\n",
    "  # Prompt for HF login *before* heavy imports\n",
    "  try:\n",
    "    from huggingface_hub import login\n",
    "    print(\"[hf] Paste your Hugging Face token, then press Enter.\")\n",
    "    login()  # notebook prompt\n",
    "  except Exception as e:\n",
    "    print(f\"[hf] login warning: {e} (continuing without explicit login)\")\n",
    "\n",
    "  prompt = _build_prompt(policy, ranking, top_k=top_k)\n",
    "\n",
    "  # Lazy import transformers; keep CPU strict\n",
    "  try:\n",
    "    os.environ[\"TRANSFORMERS_NO_TF\"] = \"True\"\n",
    "    import torch\n",
    "    torch.set_num_threads(1)  # predictable CPU usage\n",
    "  except Exception as e:\n",
    "    print(f\"[env] torch unavailable: {e}\")\n",
    "\n",
    "  try:\n",
    "    from transformers import pipeline\n",
    "  except Exception as e:\n",
    "    print(f\"[llm] transformers import failed: {e}\")\n",
    "    print(\"[llm] If needed: pip install -U transformers accelerate safetensors sentencepiece\")\n",
    "    return _fallback_plain_audit(ranking)\n",
    "\n",
    "  try:\n",
    "    print(f\"[llm] Loading pipeline (CPU): {model_id}\")\n",
    "    pipe = pipeline(\n",
    "      \"text-generation\",\n",
    "      model=model_id,\n",
    "      device_map=\"cpu\",         \n",
    "      torch_dtype=\"auto\",         \n",
    "    )\n",
    "  except Exception as e:\n",
    "    print(f\"[llm] pipeline build failed: {e}\")\n",
    "    traceback.print_exc()\n",
    "    return _fallback_plain_audit(ranking)\n",
    "\n",
    "  try:\n",
    "    outs = pipe(\n",
    "      prompt,\n",
    "      max_new_tokens=int(max_new_tokens),\n",
    "      do_sample=True,\n",
    "      temperature=float(temperature),\n",
    "      top_p=0.9,\n",
    "      truncation=True,\n",
    "      return_full_text=False,\n",
    "      pad_token_id=pipe.tokenizer.eos_token_id,\n",
    "    )\n",
    "    text = outs[0][\"generated_text\"].strip()\n",
    "    if not text.startswith(\"-\"):\n",
    "      text = \"- \" + text\n",
    "    lines = [ln.strip() for ln in text.splitlines() if ln.strip()]\n",
    "    return \"\\n\".join(lines[:7])\n",
    "  except Exception as e:\n",
    "    print(f\"[llm] generation failed: {e}\")\n",
    "    traceback.print_exc()\n",
    "    return _fallback_plain_audit(ranking)\n",
    "\n",
    "# SECTION 3 — MAIN / ENTRY\n",
    "def main():\n",
    "  ap = argparse.ArgumentParser()\n",
    "  ap.add_argument(\"--policy\", type=str, default=\"default\", choices=list(POLICIES.keys()))\n",
    "  ap.add_argument(\"--models_path\", type=str, default=\"artifacts/models.json\")\n",
    "  ap.add_argument(\"--cohort_n\", type=int, default=None, help=\"Only rank models whose test.n equals this value.\")\n",
    "  ap.add_argument(\"--model_id\", type=str, default=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
    "                  help=\"HF model id for CPU gen (e.g., TinyLlama/TinyLlama-1.1B-Chat-v1.0 or google/gemma-2-2b)\")\n",
    "  args, _ = ap.parse_known_args()\n",
    "\n",
    "  p = Path(args.models_path)\n",
    "  models = json.loads(p.read_text()) if p.exists() else DEFAULT_MODELS\n",
    "\n",
    "  result = summarize(models, args.policy, cohort_n=args.cohort_n)\n",
    "\n",
    "  print(\"# ReAct Judge Report\\n\")\n",
    "  print(f\"Policy: {args.policy}\")\n",
    "  print(f\"Hard constraints: {result['policy']['hard']}\")\n",
    "  print(f\"Weights: {result['policy']['weights']}\")\n",
    "  if result[\"cohort_n\"]:\n",
    "    print(f\"Filtered to test cohort size N={result['cohort_n']}\")\n",
    "  print()\n",
    "\n",
    "  if result[\"rejects\"]:\n",
    "    print(\"## Rejected (hard-constraint violations)\")\n",
    "    for r in result[\"rejects\"]:\n",
    "      print(r[\"explanation\"])\n",
    "    print()\n",
    "\n",
    "  if result[\"ranking\"]:\n",
    "    print(\"## Ranking (survivors)\")\n",
    "    for i, r in enumerate(result[\"ranking\"], 1):\n",
    "      t = r[\"test\"]\n",
    "      print(f\"{i}. {r['model_name']}: TOTAL={r['score']:.2f} | \"\n",
    "            f\"AUC={t.get('roc_auc',0):.3f} | F1w={t.get('f1_weighted',0):.2f} | \"\n",
    "            f\"Sens={t.get('recall_pos',0):.2f} | Spec={t.get('recall_neg',0):.2f} | \"\n",
    "            f\"Acc={t.get('accuracy',0):.2f} | OverfitGap={r['subs'].get('overfit_gap',0):.3f} | \"\n",
    "            f\"testN={t.get('n','?')}\")\n",
    "    print()\n",
    "\n",
    "    expl = _top_two_explanation(result[\"ranking\"])\n",
    "    if expl:\n",
    "      why1, why2, trade = expl\n",
    "      print(\"## Why Rank #1 and #2\")\n",
    "      print(f\"- {why1}\")\n",
    "      print(f\"- {why2}\")\n",
    "      print(f\"- {trade}\")\n",
    "      print()\n",
    "\n",
    "  note = llm_audit_note_via_hf(\n",
    "    result[\"policy\"], result[\"ranking\"],\n",
    "    model_id=args.model_id,\n",
    "    temperature=0.2,\n",
    "    max_new_tokens=280,\n",
    "    top_k=2\n",
    "  )\n",
    "  print(\"## LLM Audit Note\")\n",
    "  print(note)\n",
    "  print()\n",
    "\n",
    "  Path(\"artifacts\").mkdir(parents=True, exist_ok=True)\n",
    "  Path(\"artifacts/judge_report.json\").write_text(json.dumps(result, indent=2))\n",
    "  print(\"Saved machine-readable report to artifacts/judge_report.json\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "  main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
