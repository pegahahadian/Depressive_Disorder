{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4ba927-8698-47d1-bf83-e4ff687fd704",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, PolynomialFeatures\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.metrics import (\n",
    "    classification_report, accuracy_score, roc_auc_score, roc_curve,\n",
    "    precision_recall_curve, confusion_matrix\n",
    ")\n",
    "\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "import shap\n",
    "import xgboost as xgb\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "\n",
    "RNG = 42\n",
    "np.random.seed(RNG)\n",
    "\n",
    "# ----------------- PATHS -----------------\n",
    "DATA_PATH = r\"E:\\PXA252_BH\\OlderFiles20250512\\class_all_with_chronic_names.csv\"\n",
    "\n",
    "# 1) LOAD + BASIC CLEAN\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "\n",
    "# Harmonize binary target (1->0, 2/3->1) to handle both your binary and ternary cases\n",
    "df = df[df['class'].isin([1, 2, 3])]\n",
    "df['class'] = df['class'].replace({1: 0, 2: 1, 3: 1})\n",
    "y = df['class']\n",
    "\n",
    "drop_cols = ['HASHED_PERSONID', 'ENCNTR_ID_SI', 'DIAG_DT_TM', 'ICD', 'DIAGNOSIS_DISPLAY', 'DIAG_TYPE']\n",
    "df = df.drop(columns=[c for c in drop_cols if c in df.columns], errors='ignore')\n",
    "X_full = df.drop(columns=['class'])\n",
    "\n",
    "# Identify categorical columns on the raw data\n",
    "cat_cols_raw = X_full.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "# Replace infs with NaN uniformly\n",
    "X_full = X_full.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "# 2) PARALLEL DATA VIEWS\n",
    "#    - X_cb: for CatBoost (categoricals as string, numerics imputed only)\n",
    "#    - X_num: for XGB & LogReg (categoricals label-encoded to ints, numerics imputed)\n",
    "# ---- X_cb for CatBoost\n",
    "X_cb = X_full.copy()\n",
    "\n",
    "# Ensure categorical columns are strings with an explicit Missing token\n",
    "for col in cat_cols_raw:\n",
    "    X_cb[col] = X_cb[col].astype(str)\n",
    "    X_cb[col] = X_cb[col].replace({'nan': 'Missing'}).fillna('Missing')\n",
    "\n",
    "# Impute numeric columns (leaving categoricals as strings)\n",
    "num_cols_cb = X_cb.select_dtypes(include=[np.number]).columns\n",
    "imp_num_cb = SimpleImputer(strategy='mean')\n",
    "X_cb[num_cols_cb] = imp_num_cb.fit_transform(X_cb[num_cols_cb])\n",
    "\n",
    "# ---- X_num for XGB / LogReg\n",
    "X_num = X_full.copy()\n",
    "# Label-encode categoricals to integers\n",
    "label_encoders = {}\n",
    "for col in cat_cols_raw:\n",
    "    le = LabelEncoder()\n",
    "    X_num[col] = le.fit_transform(X_num[col].astype(str))  # encodes Missing as a distinct int\n",
    "    label_encoders[col] = le\n",
    "\n",
    "# Impute all numeric columns\n",
    "num_cols_num = X_num.select_dtypes(include=[np.number]).columns\n",
    "imp_num_num = SimpleImputer(strategy='mean')\n",
    "X_num[num_cols_num] = imp_num_num.fit_transform(X_num[num_cols_num])\n",
    "\n",
    "# 3) CONSISTENT FEATURE SELECTION (RF-based, on X_num)\n",
    "#    - Select top N features from X_num (int-coded cats + numerics)\n",
    "#    - Apply the same feature subset to X_cb (by column names)\n",
    "N_TOP = 30\n",
    "rf_fs = RandomForestClassifier(n_estimators=100, random_state=RNG)\n",
    "rf_fs.fit(X_num, y)\n",
    "top_features = pd.Series(rf_fs.feature_importances_, index=X_num.columns).nlargest(N_TOP).index.tolist()\n",
    "\n",
    "X_num = X_num[top_features].copy()\n",
    "X_cb  = X_cb[[c for c in top_features if c in X_cb.columns]].copy()  # some rare columns may be absent\n",
    "\n",
    "# Update cat_cols according to the selected features\n",
    "cat_cols_cb = [c for c in cat_cols_raw if c in X_cb.columns]\n",
    "\n",
    "# 4) SPLITS (same indices across both views for fair comparison)\n",
    "X_train_val_idx, X_test_idx, y_train_val, y_test = train_test_split(\n",
    "    np.arange(len(y)), y, stratify=y, test_size=0.2, random_state=RNG\n",
    ")\n",
    "X_train_idx, X_val_idx, y_train, y_val = train_test_split(\n",
    "    X_train_val_idx, y_train_val, stratify=y_train_val, test_size=0.25, random_state=RNG\n",
    ")\n",
    "\n",
    "def take_rows(Xdf, idx):\n",
    "    return Xdf.iloc[idx].reset_index(drop=True)\n",
    "\n",
    "# Views for CatBoost\n",
    "X_cb_train = take_rows(X_cb, X_train_idx)\n",
    "X_cb_val   = take_rows(X_cb, X_val_idx)\n",
    "X_cb_test  = take_rows(X_cb, X_test_idx)\n",
    "\n",
    "# Views for XGB/LogReg\n",
    "X_num_train = take_rows(X_num, X_train_idx)\n",
    "X_num_val   = take_rows(X_num, X_val_idx)\n",
    "X_num_test  = take_rows(X_num, X_test_idx)\n",
    "\n",
    "# Targets\n",
    "y_train = pd.Series(y_train).reset_index(drop=True)\n",
    "y_val   = pd.Series(y_val).reset_index(drop=True)\n",
    "y_test  = pd.Series(y_test).reset_index(drop=True)\n",
    "\n",
    "# 5) MODEL 1: CATBOOST (with undersampling, strict dtype handling) + SHAP\n",
    "# Undersample training to balance\n",
    "rus = RandomUnderSampler(sampling_strategy=1.0, random_state=RNG)\n",
    "X_cb_train_bal, y_train_bal = rus.fit_resample(X_cb_train, y_train)\n",
    "\n",
    "# Make sure cats stay strings everywhere (re-assert; harmless if already)\n",
    "for col in cat_cols_cb:\n",
    "    for frame in (X_cb_train_bal, X_cb_train, X_cb_val, X_cb_test):\n",
    "        frame[col] = frame[col].astype(str).fillna('Missing')\n",
    "\n",
    "# Cat feature indices relative to X_cb_train_bal columns\n",
    "cat_features_idx = [X_cb_train_bal.columns.get_loc(c) for c in cat_cols_cb if c in X_cb_train_bal.columns]\n",
    "\n",
    "# Train CatBoost\n",
    "cb_model = CatBoostClassifier(\n",
    "    iterations=500,\n",
    "    learning_rate=0.03,\n",
    "    depth=5,\n",
    "    l2_leaf_reg=8,\n",
    "    eval_metric='F1',\n",
    "    random_seed=RNG,\n",
    "    cat_features=cat_features_idx,\n",
    "    early_stopping_rounds=50,\n",
    "    class_weights={0: 1.1, 1: 1},\n",
    "    verbose=100\n",
    ")\n",
    "cb_model.fit(X_cb_train_bal, y_train_bal, eval_set=(X_cb_val, y_val), use_best_model=True)\n",
    "\n",
    "def evaluate_catboost(mdl, Xd, yd, label):\n",
    "    pool = Pool(Xd, label=yd, cat_features=cat_features_idx)\n",
    "    y_pred = mdl.predict(pool)\n",
    "    y_prob = mdl.predict_proba(pool)[:, 1]\n",
    "    print(f\"\\n📊 {label} (CatBoost) Report:\")\n",
    "    print(classification_report(yd, y_pred))\n",
    "    print(f\"✅ Acc: {accuracy_score(yd, y_pred):.4f}\")\n",
    "    print(f\"🎯 ROC-AUC: {roc_auc_score(yd, y_prob):.4f}\")\n",
    "    return yd, y_prob\n",
    "\n",
    "ytr_cb, ytrp_cb = evaluate_catboost(cb_model, X_cb_train, y_train, \"Train\")\n",
    "yva_cb,  yvap_cb = evaluate_catboost(cb_model, X_cb_val,   y_val,   \"Validation\")\n",
    "yte_cb,  ytep_cb = evaluate_catboost(cb_model, X_cb_test,  y_test,  \"Test\")\n",
    "\n",
    "# Threshold optimization (maximize recall for class 0 with precision > 0.5)\n",
    "prec, rec, thr = precision_recall_curve((yva_cb == 0).astype(int), 1 - yvap_cb)\n",
    "best_thresh, max_recall = 0.5, 0\n",
    "for p, r, t in zip(prec, rec, np.append(thr, 1.0)):\n",
    "    if p > 0.5 and r > max_recall:\n",
    "        max_recall = r\n",
    "        best_thresh = 1 - t\n",
    "optimal_thresh_cb = best_thresh\n",
    "print(f\"🔧 CatBoost class-0 recall-optimized threshold = {optimal_thresh_cb:.3f}\")\n",
    "\n",
    "\n",
    "# 6) MODEL 2: XGBOOST (with calibration) + SHAP\n",
    "class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y_train), y=y_train)\n",
    "sample_weights = np.where(y_train == 0, class_weights[0], class_weights[1])\n",
    "num_class0 = int(np.sum(y_train == 0))\n",
    "num_class1 = int(np.sum(y_train == 1))\n",
    "scale_pos_weight = num_class0 / max(1, num_class1)\n",
    "\n",
    "xgb_model = xgb.XGBClassifier(\n",
    "    n_estimators=200,\n",
    "    learning_rate=0.03,\n",
    "    max_depth=5,\n",
    "    subsample=0.6,\n",
    "    colsample_bytree=0.6,\n",
    "    reg_lambda=10,\n",
    "    reg_alpha=5,\n",
    "    scale_pos_weight=scale_pos_weight,\n",
    "    objective='binary:logistic',\n",
    "    eval_metric='auc',\n",
    "    random_state=RNG\n",
    ")\n",
    "xgb_model.fit(X_num_train, y_train, sample_weight=sample_weights)\n",
    "\n",
    "# Calibrate (sigmoid)\n",
    "xgb_cal = CalibratedClassifierCV(xgb_model, method='sigmoid', cv='prefit')\n",
    "xgb_cal.fit(X_num_val, y_val)\n",
    "\n",
    "def evaluate_prob_model(model, Xd, yd, label, threshold=0.636):\n",
    "    y_prob = model.predict_proba(Xd)[:, 1]\n",
    "    y_pred = (y_prob >= threshold).astype(int)\n",
    "    print(f\"\\n📊 {label} (XGB Calibrated) Report (thr={threshold:.3f}):\")\n",
    "    print(classification_report(yd, y_pred))\n",
    "    print(f\"✅ Acc: {accuracy_score(yd, y_pred):.4f}\")\n",
    "    print(f\"🎯 ROC-AUC: {roc_auc_score(yd, y_prob):.4f}\")\n",
    "    return yd, y_prob, y_pred\n",
    "\n",
    "THRESH_XGB = 0.636\n",
    "ytr_xgb, ytrp_xgb, _ = evaluate_prob_model(xgb_cal, X_num_train, y_train, \"Train\", THRESH_XGB)\n",
    "yva_xgb, yvap_xgb, _ = evaluate_prob_model(xgb_cal, X_num_val,   y_val,   \"Validation\", THRESH_XGB)\n",
    "yte_xgb, ytep_xgb, ytepred_xgb = evaluate_prob_model(xgb_cal, X_num_test,  y_test,  \"Test\", THRESH_XGB)\n",
    "\n",
    "# SHAP for XGB (use pre-calibration model)\n",
    "os.makedirs(\"shap_outputs/xgb\", exist_ok=True)\n",
    "xgb_explainer = shap.TreeExplainer(xgb_model)\n",
    "xgb_shap_test = xgb_explainer.shap_values(X_num_test)\n",
    "xgb_base_value = xgb_explainer.expected_value\n",
    "xgb_feature_names = list(X_num_train.columns)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "shap.summary_plot(xgb_shap_test, features=X_num_test, feature_names=xgb_feature_names, show=False)\n",
    "plt.tight_layout(); plt.savefig(\"shap_outputs/xgb/summary_beeswarm.png\", dpi=200); plt.close()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "shap.summary_plot(xgb_shap_test, features=X_num_test, feature_names=xgb_feature_names, plot_type=\"bar\", show=False)\n",
    "plt.tight_layout(); plt.savefig(\"shap_outputs/xgb/summary_bar.png\", dpi=200); plt.close()\n",
    "\n",
    "i = 0\n",
    "plt.figure(figsize=(9, 6))\n",
    "shap.plots._waterfall.waterfall_legacy(xgb_base_value, xgb_shap_test[i], feature_names=xgb_feature_names, max_display=20, show=False)\n",
    "plt.tight_layout(); plt.savefig(f\"shap_outputs/xgb/waterfall_idx{i}.png\", dpi=200); plt.close()\n",
    "\n",
    "print(\"✅ XGBoost SHAP plots saved to shap_outputs/xgb\")\n",
    "\n",
    "# 7) MODEL 3: LOGISTIC REGRESSION (Elastic-Net on poly+SelectKBest) + SHAP\n",
    "# Standardize numeric-coded view before poly\n",
    "scaler = StandardScaler()\n",
    "X_num_train_s = scaler.fit_transform(X_num_train)\n",
    "X_num_val_s   = scaler.transform(X_num_val)\n",
    "X_num_test_s  = scaler.transform(X_num_test)\n",
    "\n",
    "# Polynomial interactions (degree=2, interactions only)\n",
    "poly = PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)\n",
    "X_tr_poly = poly.fit_transform(X_num_train_s)\n",
    "X_va_poly = poly.transform(X_num_val_s)\n",
    "X_te_poly = poly.transform(X_num_test_s)\n",
    "poly_feature_names = poly.get_feature_names_out(X_num_train.columns)\n",
    "\n",
    "# Feature selection (top K)\n",
    "K = 100 if X_tr_poly.shape[1] >= 100 else X_tr_poly.shape[1]\n",
    "selector = SelectKBest(score_func=f_classif, k=K)\n",
    "X_tr_sel = selector.fit_transform(X_tr_poly, y_train)\n",
    "X_va_sel = selector.transform(X_va_poly)\n",
    "X_te_sel = selector.transform(X_te_poly)\n",
    "selected_feature_names = np.array(poly_feature_names)[selector.get_support()]\n",
    "\n",
    "# Grid search LR (Elastic-Net)\n",
    "param_grid = {\n",
    "    'C': [0.001, 0.01, 0.1, 1, 10],\n",
    "    'penalty': ['elasticnet'],\n",
    "    'l1_ratio': [0.1, 0.5, 0.9],\n",
    "    'solver': ['saga'],\n",
    "    'class_weight': [None, 'balanced'],\n",
    "    'max_iter': [5000]\n",
    "}\n",
    "grid_lr = GridSearchCV(\n",
    "    LogisticRegression(),\n",
    "    param_grid,\n",
    "    scoring='roc_auc',\n",
    "    cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=RNG),\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "grid_lr.fit(np.vstack([X_tr_sel, X_va_sel]), pd.concat([y_train, y_val]).values)\n",
    "best_lr = grid_lr.best_estimator_\n",
    "print(\"🔍 LR best params:\", grid_lr.best_params_)\n",
    "\n",
    "def evaluate_lr(model, Xd, yd, label):\n",
    "    y_pred = model.predict(Xd)\n",
    "    y_prob = model.predict_proba(Xd)[:, 1]\n",
    "    print(f\"\\n📊 {label} (LogReg) Report:\")\n",
    "    print(classification_report(yd, y_pred))\n",
    "    print(f\"✅ Acc: {accuracy_score(yd, y_pred):.4f}\")\n",
    "    print(f\"🎯 ROC-AUC: {roc_auc_score(yd, y_prob):.4f}\")\n",
    "    return yd, y_prob\n",
    "\n",
    "ytr_lr, ytrp_lr = evaluate_lr(best_lr, X_tr_sel, y_train, \"Train\")\n",
    "yva_lr, yvap_lr = evaluate_lr(best_lr, X_va_sel, y_val,   \"Validation\")\n",
    "yte_lr, ytep_lr = evaluate_lr(best_lr, X_te_sel, y_test,  \"Test\")\n",
    "\n",
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from catboost import Pool\n",
    "\n",
    "import shap, matplotlib.pyplot as plt, os\n",
    "from catboost import Pool\n",
    "\n",
    "os.makedirs(\"shap_outputs/catboost\", exist_ok=True)\n",
    "\n",
    "cb_shap_all = cb_model.get_feature_importance(\n",
    "    Pool(X_cb_test, label=y_test, cat_features=cat_features_idx),\n",
    "    type=\"ShapValues\"\n",
    ")\n",
    "\n",
    "cb_shap_values = cb_shap_all[:, :-1]   # per-feature SHAP values\n",
    "cb_expected_value = cb_shap_all[:, -1] # expected value for each sample\n",
    "\n",
    "feature_names = list(X_cb_test.columns)\n",
    "\n",
    "# --- Beeswarm ---\n",
    "plt.figure(figsize=(10, 6))\n",
    "shap.summary_plot(cb_shap_values, features=X_cb_test, feature_names=feature_names, show=False)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"shap_outputs/catboost/summary_beeswarm.png\", dpi=300)\n",
    "plt.close()\n",
    "\n",
    "# --- Bar plot ---\n",
    "plt.figure(figsize=(10, 6))\n",
    "shap.summary_plot(cb_shap_values, features=X_cb_test, feature_names=feature_names, plot_type=\"bar\", show=False)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"shap_outputs/catboost/summary_bar.png\", dpi=300)\n",
    "plt.close()\n",
    "\n",
    "# --- Local waterfall for one test example ---\n",
    "i = 0\n",
    "plt.figure(figsize=(9, 6))\n",
    "shap.plots._waterfall.waterfall_legacy(cb_expected_value[i], cb_shap_values[i],\n",
    "                                       feature_names=feature_names, max_display=20, show=False)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"shap_outputs/catboost/waterfall_idx{i}.png\", dpi=300)\n",
    "plt.close()\n",
    "\n",
    "print(\"✅ CatBoost SHAP plots generated using native get_feature_importance(type='ShapValues').\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad9ffb0-07da-4ade-a021-c1800ca49c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "# Step 1: Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 2: Standardize\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Step 3: Train Logistic Regression\n",
    "log_reg = LogisticRegression(max_iter=1000, class_weight='balanced', random_state=42)\n",
    "log_reg.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Step 4: Evaluation\n",
    "y_pred = log_reg.predict(X_test_scaled)\n",
    "print(\"✅ Logistic Regression Classification Report:\\n\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(f\"🎯 ROC-AUC: {roc_auc_score(y_test, log_reg.predict_proba(X_test_scaled)[:,1]):.4f}\")\n",
    "\n",
    "# Step 5: SHAP Explanation\n",
    "shap.initjs()\n",
    "lr_explainer = shap.LinearExplainer(log_reg, X_train_scaled, feature_perturbation=\"interventional\")\n",
    "lr_shap_values = lr_explainer.shap_values(X_test_scaled)\n",
    "\n",
    "# Optional: Save SHAP values\n",
    "with open(\"shap_values_log_reg.pkl\", \"wb\") as f:\n",
    "    pickle.dump(lr_shap_values, f)\n",
    "\n",
    "# Optional: Save explainer\n",
    "with open(\"shap_explainer_log_reg.pkl\", \"wb\") as f:\n",
    "    pickle.dump(lr_explainer, f)\n",
    "\n",
    "# Step 6: SHAP Summary Plot\n",
    "shap.summary_plot(lr_shap_values, X_test, feature_names=X.columns)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
