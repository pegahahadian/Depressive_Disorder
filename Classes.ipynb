{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61bc4b3-c17a-4694-96a2-a47ff35b4fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_diagnosis = pd.read_csv(\"Extracted_Diagnosis.csv\", dtype={'HASHED_PERSONID': str})\n",
    "df_diagnosis['DIAG_DT_TM'] = pd.to_datetime(df_diagnosis['DIAG_DT_TM'], errors='coerce')\n",
    "\n",
    "# ICD definitions\n",
    "initial_icd = \"F32.0\"\n",
    "target_icds = [\"F32.0\", \"F33.0\"]\n",
    "\n",
    "# Get initial F32.0 diagnoses\n",
    "df_initial = df_diagnosis[df_diagnosis['ICD'] == initial_icd].copy()\n",
    "\n",
    "# Merge with all diagnoses for follow-up\n",
    "df_followup = pd.merge(df_initial, df_diagnosis, on='HASHED_PERSONID', suffixes=('_initial', '_followup'))\n",
    "\n",
    "#  Filter follow-ups BETWEEN 1 year and 1.5 years later\n",
    "df_followup = df_followup[\n",
    "(df_followup['DIAG_DT_TM_followup'] > df_followup['DIAG_DT_TM_initial'] + pd.DateOffset(years=1)) &\n",
    "(df_followup['DIAG_DT_TM_followup'] <= df_followup['DIAG_DT_TM_initial'] + pd.DateOffset(months=24))\n",
    "\n",
    "]\n",
    "\n",
    "# Only keep those who were diagnosed with F32.0 or F33.0 at follow-up\n",
    "df_followup = df_followup[df_followup['ICD_followup'].isin(target_icds)]\n",
    "\n",
    "# Get unique patient IDs\n",
    "final_patient_ids = df_followup['HASHED_PERSONID'].unique()\n",
    "\n",
    "# Keep all records of interest\n",
    "df_final = df_diagnosis[\n",
    "    (df_diagnosis['HASHED_PERSONID'].isin(final_patient_ids)) &\n",
    "    (df_diagnosis['ICD'].isin(target_icds))\n",
    "].copy()\n",
    "\n",
    "# Label class = 1\n",
    "df_final['class'] = 1\n",
    "\n",
    "# Save results\n",
    "df_final.to_csv(\"class1.csv\", index=False)\n",
    "\n",
    "# Print summary\n",
    "print(f\"âœ… Saved class1.csv with {df_final['HASHED_PERSONID'].nunique()} unique patients\")\n",
    "print(\"ðŸ“‹ Columns:\", list(df_final.columns))\n",
    "print(\"ðŸ”Ž Preview:\")\n",
    "print(df_final.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66258668-5bb8-483c-bbcd-2845e0f31513",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the diagnosis dataset\n",
    "df_diagnosis = pd.read_csv(\"Extracted_Diagnosis.csv\", dtype={'HASHED_PERSONID': str})\n",
    "df_diagnosis['DIAG_DT_TM'] = pd.to_datetime(df_diagnosis['DIAG_DT_TM'], errors='coerce')\n",
    "\n",
    "# Define the ICDs\n",
    "worse_icds = [\"F32.1\", \"F32.2\", \"F32.3\", \"F33.1\", \"F33.2\", \"F33.3\"]\n",
    "initial_icds = [\"F32.0\", \"F33.0\"]\n",
    "\n",
    "# Helper function to identify patients who progressed in a given time window\n",
    "def get_progressed_patients(df, initial_icd, min_months, max_months, class_label):\n",
    "    df_initial = df[df['ICD'] == initial_icd].sort_values(by=['HASHED_PERSONID', 'DIAG_DT_TM'])\n",
    "    df_initial = df_initial.drop_duplicates(subset='HASHED_PERSONID', keep='first')\n",
    "\n",
    "    df_progressed = df[df['ICD'].isin(worse_icds)]\n",
    "\n",
    "    df_merged = pd.merge(\n",
    "        df_initial[['HASHED_PERSONID', 'DIAG_DT_TM']],\n",
    "        df_progressed,\n",
    "        on='HASHED_PERSONID',\n",
    "        suffixes=('_initial', '_progressed')\n",
    "    )\n",
    "\n",
    "    min_offset = pd.DateOffset(months=min_months)\n",
    "    max_offset = pd.DateOffset(months=max_months)\n",
    "\n",
    "    df_merged = df_merged[\n",
    "        (df_merged['DIAG_DT_TM_progressed'] > df_merged['DIAG_DT_TM_initial'] + min_offset) &\n",
    "        (df_merged['DIAG_DT_TM_progressed'] <= df_merged['DIAG_DT_TM_initial'] + max_offset)\n",
    "    ]\n",
    "\n",
    "    final_ids = df_merged['HASHED_PERSONID'].unique()\n",
    "\n",
    "    df_result = df[\n",
    "        (df['HASHED_PERSONID'].isin(final_ids)) &\n",
    "        (df['ICD'].isin([initial_icd] + worse_icds))\n",
    "    ].copy()\n",
    "\n",
    "    df_result['class'] = class_label\n",
    "    return df_result\n",
    "\n",
    "# Get Class 2 (within 12 months)\n",
    "df_class2_f32 = get_progressed_patients(df_diagnosis, \"F32.0\", 0, 12, 2)\n",
    "df_class2_f33 = get_progressed_patients(df_diagnosis, \"F33.0\", 0, 12, 2)\n",
    "\n",
    "# Get Class 3 (12 to 24 months)\n",
    "df_class3_f32 = get_progressed_patients(df_diagnosis, \"F32.0\", 12, 24, 3)\n",
    "df_class3_f33 = get_progressed_patients(df_diagnosis, \"F33.0\", 12, 24, 3)\n",
    "\n",
    "# Combine classes\n",
    "df_combined = pd.concat([\n",
    "    df_class2_f32, df_class2_f33,\n",
    "    df_class3_f32, df_class3_f33\n",
    "], ignore_index=True)\n",
    "\n",
    "# Save to file\n",
    "# Save to file\n",
    "df_combined.to_csv(\"class2.csv\", index=False)\n",
    "\n",
    "# âœ… Use df_combined for final reporting\n",
    "print(f\"âœ… Saved: class2.csv\")\n",
    "print(\"ðŸ‘¥ Unique patients:\", df_combined['HASHED_PERSONID'].nunique())\n",
    "print(\"ðŸ“Š Class distribution:\\n\", df_combined['class'].value_counts())\n",
    "print(\"ðŸ” ICD codes included:\", df_combined['ICD'].unique())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766c7743-3892-4228-9682-5954fe35c041",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv(\"class_all_with_chronic_names.csv\")\n",
    "\n",
    "# Drop unnecessary identifier columns if needed\n",
    "cols_to_drop = ['HASHED_PERSONID', 'ENCNTR_ID_SI', 'DIAG_DT_TM', 'ICD', 'DIAGNOSIS_DISPLAY']\n",
    "df = df.drop(columns=cols_to_drop, errors='ignore')\n",
    "\n",
    "# Separate features and target\n",
    "y = df['class']\n",
    "X = df.drop(columns=['class'])\n",
    "\n",
    "# Label encode categorical features\n",
    "for col in X.select_dtypes(include=['object', 'category']).columns:\n",
    "    le = LabelEncoder()\n",
    "    X[col] = le.fit_transform(X[col].astype(str))\n",
    "\n",
    "# Handle missing values\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X_array = imputer.fit_transform(X)\n",
    "X = pd.DataFrame(X_array, columns=X.columns)\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train Random Forest\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Get feature importances\n",
    "importances = pd.Series(rf.feature_importances_, index=X.columns).sort_values(ascending=False)\n",
    "\n",
    "# Plot all features\n",
    "plt.figure(figsize=(10, len(importances) * 0.25))\n",
    "importances.plot(kind='barh')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.title(\"Feature Importances (Random Forest)\")\n",
    "plt.xlabel(\"Importance\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print all feature importances\n",
    "for feature, importance in importances.items():\n",
    "    print(f\"{feature}: {importance:.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a8dd8a7-9fb9-4b55-9dbb-3474c6ede792",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Step 1: Load dataset\n",
    "df = pd.read_csv(\"class_all_with_chronic_names.csv\")\n",
    "\n",
    "# Step 2: Drop unnecessary columns\n",
    "drop_cols = ['HASHED_PERSONID', 'ENCNTR_ID_SI', 'DIAG_DT_TM', 'ICD', 'DIAGNOSIS_DISPLAY']\n",
    "df.drop(columns=[col for col in drop_cols if col in df.columns], inplace=True)\n",
    "\n",
    "# Step 3: Drop rows with missing target\n",
    "# Step 3: Remove rows with missing or unexpected class values\n",
    "df = df[df['class'].isin([1, 2])]\n",
    "y = df['class'].map({1: 0, 2: 1})\n",
    "\n",
    "# Optional: check class balance\n",
    "print(\"âœ… Class distribution:\\n\", y.value_counts())\n",
    "\n",
    "\n",
    "# Step 4: Separate features and target\n",
    "y = df['class'].map({1: 0, 2: 1})  # Binary conversion\n",
    "X = df.drop(columns=['class'])\n",
    "\n",
    "# Step 5: Label encode categorical columns\n",
    "for col in X.select_dtypes(include=['object', 'category']).columns:\n",
    "    le = LabelEncoder()\n",
    "    X[col] = le.fit_transform(X[col].astype(str))\n",
    "\n",
    "# Step 6: Convert infinite to NaN and impute\n",
    "X.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "missing_before = X.isnull().sum().sum()\n",
    "print(f\"ðŸ§¼ Missing values before imputation: {missing_before}\")\n",
    "\n",
    "# Step 7: Impute missing values\n",
    "imputer = SimpleImputer(strategy=\"mean\")\n",
    "X_imputed = pd.DataFrame(imputer.fit_transform(X), columns=X.columns)\n",
    "\n",
    "# Step 8: Final check\n",
    "X_clean = X_imputed.copy()\n",
    "if X_clean.isnull().sum().sum() == 0 and np.isfinite(X_clean.values).all():\n",
    "    print(\"âœ… Data is clean and ready.\")\n",
    "else:\n",
    "    raise ValueError(\"âŒ Still NaNs or Infs in the dataset after cleaning!\")\n",
    "\n",
    "# Step 9: Feature Selection with Random Forest\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf.fit(X_clean, y)\n",
    "importances = pd.Series(rf.feature_importances_, index=X_clean.columns)\n",
    "top_20_features = importances.sort_values(ascending=False).head(40).index.tolist()\n",
    "\n",
    "# Step 10: Subset top 20 features\n",
    "X_top = X_clean[top_20_features]\n",
    "\n",
    "# Step 11: Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_top, y, test_size=0.2, stratify=y, random_state=42)\n",
    "\n",
    "# Step 12: Train XGBoost\n",
    "xgb = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
    "xgb.fit(X_train, y_train)\n",
    "\n",
    "# Step 13: Evaluation\n",
    "y_pred = xgb.predict(X_test)\n",
    "print(\"ðŸ“Š Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "print(\"âœ… Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "\n",
    "# Step 14: Plot\n",
    "importances_xgb = pd.Series(xgb.feature_importances_, index=X_top.columns).sort_values()\n",
    "plt.figure(figsize=(10, 8))\n",
    "importances_xgb.plot(kind='barh')\n",
    "plt.title(\"XGBoost Feature Importances (Top 40)\")\n",
    "plt.xlabel(\"Importance\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
